{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "15M-Ne4v2BZJ"
   },
   "source": [
    "# **Experiment Demo Notebook**\n",
    "\n",
    "This notebook has been prepared to allow for the direct running of our experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fS6tCMz92dDQ"
   },
   "source": [
    "## **Setup**\n",
    "\n",
    "To first run the notebooks, we check whether we're currently using Google Colab or not. We can do so by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "nO7hv5rNsj8K",
    "outputId": "08f5d88b-3d23-4b56-9ec8-63f3b10f39bf"
   },
   "outputs": [],
   "source": [
    "# Check if using Colab or not\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    from getpass import getpass\n",
    "    from google.colab import drive # Access the drive\n",
    "    drive.mount('/content/drive')\n",
    "    pat = ''\n",
    "    repo_name = 'stefanvasilev/egnn-transformer'\n",
    "    url = f\"https://{pat}@github.com/{repo_name}.git\"\n",
    "    # !git clone --branch nbody_transformer {url}\n",
    "    !git clone --branch main {url}\n",
    "    print(\"\\nCurrent Directory:\")\n",
    "    %cd egnn-transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nIoYW-G_3EUq"
   },
   "source": [
    "Now we can setup the environment and location. If you're not using Colab, then you need to install the environment yourself (which can be done using one of the `.yml` files, which depends on your device)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "KC13ZDvktm2E",
    "outputId": "d0a0bea0-f1ce-4c78-9c7e-5f2f82c7cbd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Directory c:\\Users\\Gregory Go\\.github\\egnn-transformer\\notebooks\n",
      "c:\\Users\\Gregory Go\\.github\\egnn-transformer\n",
      "New Current Directory: c:\\Users\\Gregory Go\\.github\\egnn-transformer\\notebooks\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Read the requirements.txt file\n",
    "    with open('requirements.txt') as f:\n",
    "        requirements = f.read().splitlines()\n",
    "\n",
    "    # Check if each requirement is installed, if not, install it\n",
    "    import pkg_resources\n",
    "    installed_packages = {pkg.key for pkg in pkg_resources.working_set}\n",
    "    for requirement in requirements:\n",
    "        if not any(requirement.split('==')[0] in pkg for pkg in installed_packages):\n",
    "            !pip install {requirement}\n",
    "\n",
    "    !pip install datasets\n",
    "\n",
    "\n",
    "else: # automatically checks if the current directory is 'repo name'\n",
    "    curdir = Path.cwd()\n",
    "    print(\"Current Directory\", curdir)\n",
    "    assert curdir.name == \"egnn-transformer\" or curdir.parent.name == \"egnn-transformer\", \"Notebook cwd has to be on the project root\"\n",
    "    if curdir.name == \"notebooks\":\n",
    "        %cd ..\n",
    "        print(\"New Current Directory:\", curdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6mFBlJdb40OC"
   },
   "source": [
    "### **Generating the N-body Dataset**\n",
    "\n",
    "As the N-body dataset is made via simulation, it's possible to create it manually (and should not take long). We can do that by running the following script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "np1Qlr5O59Hl",
    "ExecuteTime": {
     "end_time": "2024-05-28T21:18:14.399369253Z",
     "start_time": "2024-05-28T21:18:13.526164897Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: can't open file '/home/jonas/Desktop/DL2/egnn-transformer/notebooks/generate_dataset.py': [Errno 2] No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!python ../n_body/dataset/generate_dataset.py --initial_vel 1 --num-train 3000 --length 1000 --length_test 1000 --sufix \"small\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h_OFrcKw6RmJ"
   },
   "source": [
    "The data for N-body should now be in `n_body/dataset/data`. Alternatively, you can download the dataset [here](https://drive.google.com/drive/folders/1xfigu6ZJHvw7smx4J_-p8uRryIYGUjK7?usp=sharing).\n",
    "\n",
    "Note that the data for QM9 is already available from `torch_geometric`, meaning that nothing else needs to be done for it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1cVZEpvH-qCO"
   },
   "source": [
    "## **Experiments**\n",
    "\n",
    "Now we can begin with the experiments. We first show how to perform those for QM9.\n",
    "\n",
    "Note that the parameters which can be adjusted are documented in the `README` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xzn0n13G_RCM"
   },
   "source": [
    "For the EGNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "UENxlUOP-2HA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set as 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Tensorflow library not found, tensorflow.io.gfile operations will use native shim calls. GCS paths (i.e. 'gs://...') cannot be accessed.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Gregory Go\\.github\\egnn-transformer\\main_qm9.py\", line 289, in <module>\n",
      "    model = get_model(parsed_args)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Gregory Go\\.github\\egnn-transformer\\utils\\utils.py\", line 150, in get_model\n",
      "    from models.egnn_jax import EGNN\n",
      "ImportError: cannot import name 'EGNN' from 'models.egnn_jax' (c:\\Users\\Gregory Go\\.github\\egnn-transformer\\models\\egnn_jax.py)\n"
     ]
    }
   ],
   "source": [
    "!python ../main_qm9.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qnPBMBcf_QC3"
   },
   "source": [
    "For the Transformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QIm-NKPoAINc"
   },
   "outputs": [],
   "source": [
    "!python ../transformer_trainer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rwi_E3aT_xIX"
   },
   "source": [
    "...and now for N-body."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "N-body EGNN (for colab it is best to use the colab notebook)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set as 42\r\n",
      "Parameters: 117575\r\n",
      "[Epoch  1] Training mse: 0.009367193095386028, Validation mse: 0.008887327276170254\r\n",
      "\t   (New best performance, saving model...)\r\n",
      "Figure(1000x600)\r\n",
      "[Epoch  2] Training mse: 0.008097128011286259, Validation mse: 0.007526990957558155\r\n",
      "\t   (New best performance, saving model...)\r\n",
      "[Epoch  3] Training mse: 0.006653978023678064, Validation mse: 0.006075744982808828\r\n",
      "\t   (New best performance, saving model...)\r\n",
      "[Epoch  4] Training mse: 0.005744718946516514, Validation mse: 0.00560001889243722\r\n",
      "\t   (New best performance, saving model...)\r\n",
      "^C\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/home/jonas/Desktop/DL2/egnn-transformer/notebooks/../nbody_egnn_trainer.py\", line 297, in <module>\r\n",
      "    train_model(parsed_args, batch_transform, \"test\", \"assets\")\r\n",
      "  File \"/home/jonas/Desktop/DL2/egnn-transformer/notebooks/../nbody_egnn_trainer.py\", line 197, in train_model\r\n",
      "    test_loss = eval_fn(test_loader, params)\r\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/home/jonas/Desktop/DL2/egnn-transformer/notebooks/../nbody_egnn_trainer.py\", line 129, in evaluate\r\n",
      "    feat, target = graph_transform(data)\r\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/home/jonas/Desktop/DL2/egnn-transformer/n_body/utils.py\", line 74, in _to_egnn\r\n",
      "    loc_dist = jnp.expand_dims(jnp.sum((pos[rows] - pos[cols]) ** 2, 1), axis=1)\r\n",
      "                                                    ~~~^^^^^^\r\n",
      "  File \"/home/jonas/miniconda3/envs/egnn-transformer/lib/python3.11/site-packages/jax/_src/array.py\", line 348, in __getitem__\r\n",
      "    return lax_numpy._rewriting_take(self, idx)\r\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/home/jonas/miniconda3/envs/egnn-transformer/lib/python3.11/site-packages/jax/_src/numpy/lax_numpy.py\", line 4604, in _rewriting_take\r\n",
      "    return _gather(arr, treedef, static_idx, dynamic_idx, indices_are_sorted,\r\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/home/jonas/miniconda3/envs/egnn-transformer/lib/python3.11/site-packages/jax/_src/numpy/lax_numpy.py\", line 4612, in _gather\r\n",
      "    idx = _merge_static_and_dynamic_indices(treedef, static_idx, dynamic_idx)\r\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/home/jonas/miniconda3/envs/egnn-transformer/lib/python3.11/site-packages/jax/_src/numpy/lax_numpy.py\", line 4713, in _merge_static_and_dynamic_indices\r\n",
      "    return treedef.unflatten(idx)\r\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "KeyboardInterrupt\r\n"
     ]
    }
   ],
   "source": [
    "!python ../nbody_egnn_trainer.py --nbody_path ../n_body/dataset/data/"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-28T21:10:25.267948975Z",
     "start_time": "2024-05-28T21:10:03.599196180Z"
    }
   },
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ablation studies Equivariance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Tensorflow library not found, tensorflow.io.gfile operations will use native shim calls. GCS paths (i.e. 'gs://...') cannot be accessed.\r\n",
      "Random seed set as 42\r\n",
      "Parameters: 92931\r\n",
      "Epoch 1:   0%|                                            | 0/6 [00:00<?, ?it/s]^C\r\n"
     ]
    }
   ],
   "source": [
    "!python ../nbody_transformer_trainer.py --nbody_path '../n_body/dataset/data/' --equivariance \"roto_translation\" --batch_size 500 --epochs 40"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-28T21:16:18.267944968Z",
     "start_time": "2024-05-28T21:15:59.092881901Z"
    }
   },
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "!python ../nbody_transformer_trainer.py --nbody_path '../n_body/dataset/data/' --equivariance \"velo_roto_translation\" --batch_size 500 --epochs 40"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "!python ../nbody_transformer_trainer.py --nbody_path '../n_body/dataset/data/' --equivariance \"translation\" --batch_size 500 --epochs 40"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "!python ../nbody_transformer_trainer.py --nbody_path '../n_body/dataset/data/' --equivariance \"not_equivariant\" --batch_size 500 --epochs 40"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ablation studies transformer architecture"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "!python ../nbody_transformer_trainer.py --nbody_path '../n_body/dataset/data/' --equivariance \"velo_roto_translation\" --batch_size 500 --epochs 40 --num_edge_encoders 0 --num_node_encoders 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "!python ../nbody_transformer_trainer.py --nbody_path '../n_body/dataset/data/' --equivariance \"velo_roto_translation\" --batch_size 500 --epochs 40 --node_only --num_node_encoders 4"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "!python ../nbody_transformer_trainer.py --nbody_path '../n_body/dataset/data/' --equivariance \"velo_roto_translation\" --batch_size 500 --epochs 40 --node_only --dim 128"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "!python ../nbody_transformer_trainer.py --nbody_path '../n_body/dataset/data/' --equivariance \"velo_roto_translation\" --batch_size 500 --epochs 40 --dim 128"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
